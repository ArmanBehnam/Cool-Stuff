---

title: Optimizers

keywords: fastai
sidebar: home_sidebar

summary: "This contains a set of optimizers."
description: "This contains a set of optimizers."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/009_optimizers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#old</span>
<span class="c1"># Lookahead implementation from https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/lookahead.py</span>

<span class="sd">&quot;&quot;&quot; Lookahead Optimizer Wrapper.</span>
<span class="sd">Implementation modified from: https://github.com/alphadl/lookahead.pytorch</span>
<span class="sd">Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="k">class</span> <span class="nc">Lookahead</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_optimizer</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid slow update rate: </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid lookahead steps: </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lookahead_alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">lookahead_k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">lookahead_step</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span> <span class="o">=</span> <span class="n">base_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span> <span class="o">=</span> <span class="n">base_optimizer</span><span class="o">.</span><span class="n">defaults</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="c1"># manually add our defaults to the param groups</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">default</span> <span class="ow">in</span> <span class="n">defaults</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_slow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">fast_p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">fast_p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">fast_p</span><span class="p">]</span>
            <span class="k">if</span> <span class="s1">&#39;slow_buffer&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_state</span><span class="p">:</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">fast_p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">fast_p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">slow</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span>
            <span class="n">slow</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;lookahead_alpha&#39;</span><span class="p">],</span> <span class="n">fast_p</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">slow</span><span class="p">)</span>
            <span class="n">fast_p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">slow</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sync_lookahead</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_slow</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># print(self.k)</span>
        <span class="c1">#assert id(self.param_groups) == id(self.base_optimizer.param_groups)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lookahead_step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lookahead_step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lookahead_k&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_slow</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fast_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">slow_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">k</span><span class="p">):</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="n">fast_state</span> <span class="o">=</span> <span class="n">fast_state_dict</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span>
        <span class="n">param_groups</span> <span class="o">=</span> <span class="n">fast_state_dict</span><span class="p">[</span><span class="s1">&#39;param_groups&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;state&#39;</span><span class="p">:</span> <span class="n">fast_state</span><span class="p">,</span>
            <span class="s1">&#39;slow_state&#39;</span><span class="p">:</span> <span class="n">slow_state</span><span class="p">,</span>
            <span class="s1">&#39;param_groups&#39;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">fast_state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;state&#39;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">],</span>
            <span class="s1">&#39;param_groups&#39;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;param_groups&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">fast_state_dict</span><span class="p">)</span>

        <span class="c1"># We want to restore the slow state, but share param_groups reference</span>
        <span class="c1"># with base_optimizer. This is a bit redundant but least code</span>
        <span class="n">slow_state_new</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="s1">&#39;slow_state&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading state_dict from optimizer without Lookahead applied.&#39;</span><span class="p">)</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;slow_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
            <span class="n">slow_state_new</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">slow_state_dict</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;state&#39;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;slow_state&#39;</span><span class="p">],</span>
            <span class="s1">&#39;param_groups&#39;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s1">&#39;param_groups&#39;</span><span class="p">],</span>  <span class="c1"># this is pointless but saves code</span>
        <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lookahead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">slow_state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_optimizer</span><span class="o">.</span><span class="n">param_groups</span>  <span class="c1"># make both ref same container</span>
        <span class="k">if</span> <span class="n">slow_state_new</span><span class="p">:</span>
            <span class="c1"># reapply defaults to catch missing lookahead specific ones</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">default</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">defaults</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                    <span class="n">group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">LookaheadWrapper</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">opt_func</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Lookahead</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RALAMB" class="doc_header"><code>class</code> <code>RALAMB</code><a href="https://github.com/timeseriesAI/timeseriesAI/tree/master/tsai/optimizers.py#L13" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RALAMB</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>betas</code></strong>=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="Over9000" class="doc_header"><code>Over9000</code><a href="https://github.com/timeseriesAI/timeseriesAI/tree/master/tsai/optimizers.py#L110" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>Over9000</code>(<strong><code>params</code></strong>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>5</code></em>, <strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Ranger" class="doc_header"><code>class</code> <code>Ranger</code><a href="https://github.com/timeseriesAI/timeseriesAI/tree/master/tsai/optimizers.py#L146" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Ranger</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>6</code></em>, <strong><code>N_sma_threshhold</code></strong>=<em><code>5</code></em>, <strong><code>betas</code></strong>=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-05</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>use_gc</code></strong>=<em><code>True</code></em>, <strong><code>gc_conv_only</code></strong>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

