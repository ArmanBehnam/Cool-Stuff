{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.ResCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResCNN\n",
    "\n",
    "> This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n",
    "* Zou, X., Wang, Z., Li, Q., & Sheng, W. (2019). Integration of residual network and convolutional neural network along with various activation functions and global pooling for time series classification. Neurocomputing.\n",
    "* No official implementation found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Block(Module):\n",
    "    def __init__(self, ni, nf, ks=[7, 5, 3], act_fn='relu'):\n",
    "        self.conv1 = Conv1d(ni, nf, ks[0], padding='same', act_fn=act_fn)\n",
    "        self.conv2 = Conv1d(nf, nf, ks[1], padding='same', act_fn=act_fn)\n",
    "        self.conv3 = Conv1d(nf, nf, ks[2], padding='same', act_fn=False)\n",
    "\n",
    "        # expand channels for the sum if necessary\n",
    "        self.shortcut = noop if ni == nf else Conv1d(ni, nf, ks=1, act_fn=False)\n",
    "        self.act_fn = get_act_layer(act_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        sc = self.shortcut(res)\n",
    "        x += sc\n",
    "        x = self.act_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResCNN(Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        nf = 64\n",
    "        self.block = Block(c_in, nf, ks=[7, 5, 3], act_fn='relu')\n",
    "        self.conv1 = Conv1d(nf, nf * 2, ks=3, padding='same', act_fn='leakyrelu', act_kwargs={'negative_slope':.2})\n",
    "        self.conv2 = Conv1d(nf * 2, nf * 4, ks=3, padding='same', act_fn='prelu')\n",
    "        self.conv3 = Conv1d(nf * 4, nf * 2, ks=3, padding='same', act_fn='elu', act_kwargs={'alpha':.3})\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.lin = nn.Linear(nf * 2, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResCNN(\n",
       "  (block): Block(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv1dSame(\n",
       "        (conv1d_same): Conv1d(3, 64, kernel_size=(7,), stride=(1,))\n",
       "      )\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv1dSame(\n",
       "        (conv1d_same): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "      )\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv1dSame(\n",
       "        (conv1d_same): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "      )\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (shortcut): Sequential(\n",
       "      (0): Conv1dSame(\n",
       "        (conv1d_same): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_fn): ReLU()\n",
       "  )\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1dSame(\n",
       "      (conv1d_same): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv1dSame(\n",
       "      (conv1d_same): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv1dSame(\n",
       "      (conv1d_same): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=0.3)\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "  (lin): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = torch.rand(16, 3, 128)\n",
    "test_eq(ResCNN(3,2)(xb).shape, [xb.shape[0], 2])\n",
    "ResCNN(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
