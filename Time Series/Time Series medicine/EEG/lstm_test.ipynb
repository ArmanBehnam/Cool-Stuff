{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Testing LSTM for Dog 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils.construct_save_tensors import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read pregenerated tensors\n",
    "# Dog 1\n",
    "X = np.load(\"../data/lstm/Dog_1_X.npy\")\n",
    "Y = np.load(\"../data/lstm/Dog_1_Y.npy\")\n",
    "X_test = np.load(\"../data/lstm/Dog_1_test.npy\")\n",
    "clip_ids = np.load(\"../data/lstm/Dog_1_clip_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape:  (3024, 200, 16)\n"
     ]
    }
   ],
   "source": [
    "print (\"Tensor shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, normalize by maximum and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "X = X / np.max(np.abs(X), axis=1)[:,None,:]\n",
    "\n",
    "# Shuffle to break time order \n",
    "np.random.seed(1)\n",
    "shuffle = np.random.choice(np.arange(len(Y)), size=len(Y), replace=False)\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "clip_ids = clip_ids[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from sklearn.metrics import roc_auc_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Example hyperparameters\n",
    "lstm_size = 32         # Number of neurons in each LSTM cell\n",
    "lstm_layers = 2        # Number of LSTM layers\n",
    "batch_size = 200       # Obs. per batch\n",
    "seq_len = 200          # Sequence length\n",
    "learning_rate = 0.0001 # Learning rate\n",
    "epochs = 500           # Number of epochs\n",
    "\n",
    "# Fixed param, number of channels\n",
    "n_channels = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initiate graph and add placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initiate graph object\n",
    "graph = tf.Graph() \n",
    "\n",
    "# Construct placeholders\n",
    "# inputs_ : (n_batch, seq_len, n_channels)\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, None, n_channels], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None,1], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct inputs to LSTM layers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Construct input to LSTM layers\n",
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # swap seq_len and batch_size\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # reshape to rank 2\n",
    "\n",
    "    # Linear activation\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, tf.nn.relu)\n",
    "\n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    #initial_state =  cell.zero_state(batch_size, tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define forward pass, cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Forward pass and outputs\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32) # initial_state = initial_state\n",
    "    \n",
    "    # We only need the last output tensor to pass into predictions\n",
    "    logits = tf.layers.dense(outputs[-1], 1)\n",
    "    predictions = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # Cost function\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels_, logits = logits))\n",
    "    #cost = tf.losses.mean_squared_error(labels = labels_, predictions = predictions)   \n",
    "    #cost = tf.losses.log_loss(labels = labels_, predictions=predictions)\n",
    "    cost = tf.losses.sigmoid_cross_entropy(multi_class_labels = labels_, logits = logits)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Batching iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Construct a generator object to split the data into batches\n",
    "def get_batches(X, y, batch_size = 100):\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size] # Cut the end to have an integert multiple of batch_size\n",
    "\n",
    "    # Loop over bacthes and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size][:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split into training and validaton sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, stratify=Y, test_size = 0.2, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train the LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 Iteration: 5 Train loss: 0.653358519077301\n",
      "Epoch: 0/500 Iteration: 10 Train loss: 0.6409740447998047\n",
      "Epoch: 1/500 Iteration: 15 Train loss: 0.6335064172744751\n",
      "Epoch: 1/500 Iteration: 20 Train loss: 0.6131297945976257\n",
      "Epoch: 2/500 Iteration: 25 Train loss: 0.607958197593689\n",
      "Validation AUC: 0.5434626436781609\n",
      "Epoch: 2/500 Iteration: 30 Train loss: 0.592004656791687\n",
      "Epoch: 2/500 Iteration: 35 Train loss: 0.586287796497345\n",
      "Epoch: 3/500 Iteration: 40 Train loss: 0.5770770907402039\n",
      "Epoch: 3/500 Iteration: 45 Train loss: 0.5623218417167664\n",
      "Epoch: 4/500 Iteration: 50 Train loss: 0.5459467768669128\n",
      "Validation AUC: 0.5590277777777778\n",
      "Epoch: 4/500 Iteration: 55 Train loss: 0.520070493221283\n",
      "Epoch: 4/500 Iteration: 60 Train loss: 0.5159249901771545\n",
      "Epoch: 5/500 Iteration: 65 Train loss: 0.49209892749786377\n",
      "Epoch: 5/500 Iteration: 70 Train loss: 0.4607657194137573\n",
      "Epoch: 6/500 Iteration: 75 Train loss: 0.45767611265182495\n",
      "Validation AUC: 0.5600454980842912\n",
      "Epoch: 6/500 Iteration: 80 Train loss: 0.4276481568813324\n",
      "Epoch: 7/500 Iteration: 85 Train loss: 0.4135277271270752\n",
      "Epoch: 7/500 Iteration: 90 Train loss: 0.394336998462677\n",
      "Epoch: 7/500 Iteration: 95 Train loss: 0.360818088054657\n",
      "Epoch: 8/500 Iteration: 100 Train loss: 0.34935951232910156\n",
      "Validation AUC: 0.5542983716475095\n",
      "Epoch: 8/500 Iteration: 105 Train loss: 0.317928671836853\n",
      "Epoch: 9/500 Iteration: 110 Train loss: 0.2993907630443573\n",
      "Epoch: 9/500 Iteration: 115 Train loss: 0.2205725908279419\n",
      "Epoch: 9/500 Iteration: 120 Train loss: 0.28520891070365906\n",
      "Epoch: 10/500 Iteration: 125 Train loss: 0.21175573766231537\n",
      "Validation AUC: 0.5545378352490422\n",
      "Epoch: 10/500 Iteration: 130 Train loss: 0.22643834352493286\n",
      "Epoch: 11/500 Iteration: 135 Train loss: 0.23896165192127228\n",
      "Epoch: 11/500 Iteration: 140 Train loss: 0.2176973670721054\n",
      "Epoch: 12/500 Iteration: 145 Train loss: 0.25646886229515076\n",
      "Epoch: 12/500 Iteration: 150 Train loss: 0.25217822194099426\n",
      "Validation AUC: 0.5573515325670498\n",
      "Epoch: 12/500 Iteration: 155 Train loss: 0.258317768573761\n",
      "Epoch: 13/500 Iteration: 160 Train loss: 0.22452005743980408\n",
      "Epoch: 13/500 Iteration: 165 Train loss: 0.18611553311347961\n",
      "Epoch: 14/500 Iteration: 170 Train loss: 0.23128168284893036\n",
      "Epoch: 14/500 Iteration: 175 Train loss: 0.12979325652122498\n",
      "Validation AUC: 0.5581896551724137\n",
      "Epoch: 14/500 Iteration: 180 Train loss: 0.22714117169380188\n",
      "Epoch: 15/500 Iteration: 185 Train loss: 0.15259148180484772\n",
      "Epoch: 15/500 Iteration: 190 Train loss: 0.19109086692333221\n",
      "Epoch: 16/500 Iteration: 195 Train loss: 0.22002525627613068\n",
      "Epoch: 16/500 Iteration: 200 Train loss: 0.18925490975379944\n",
      "Validation AUC: 0.5609434865900383\n",
      "Epoch: 17/500 Iteration: 205 Train loss: 0.2369808554649353\n",
      "Epoch: 17/500 Iteration: 210 Train loss: 0.24733152985572815\n",
      "Epoch: 17/500 Iteration: 215 Train loss: 0.2531275749206543\n",
      "Epoch: 18/500 Iteration: 220 Train loss: 0.23907609283924103\n",
      "Epoch: 18/500 Iteration: 225 Train loss: 0.18834714591503143\n",
      "Validation AUC: 0.5642361111111112\n",
      "Epoch: 19/500 Iteration: 230 Train loss: 0.21259939670562744\n",
      "Epoch: 19/500 Iteration: 235 Train loss: 0.10730338096618652\n",
      "Epoch: 19/500 Iteration: 240 Train loss: 0.23932918906211853\n",
      "Epoch: 20/500 Iteration: 245 Train loss: 0.15875983238220215\n",
      "Epoch: 20/500 Iteration: 250 Train loss: 0.16687139868736267\n",
      "Validation AUC: 0.5669899425287356\n",
      "Epoch: 21/500 Iteration: 255 Train loss: 0.22038151323795319\n",
      "Epoch: 21/500 Iteration: 260 Train loss: 0.18619072437286377\n",
      "Epoch: 22/500 Iteration: 265 Train loss: 0.21744871139526367\n",
      "Epoch: 22/500 Iteration: 270 Train loss: 0.23683056235313416\n",
      "Epoch: 22/500 Iteration: 275 Train loss: 0.26405420899391174\n",
      "Validation AUC: 0.5700431034482759\n",
      "Epoch: 23/500 Iteration: 280 Train loss: 0.22966066002845764\n",
      "Epoch: 23/500 Iteration: 285 Train loss: 0.20080691576004028\n",
      "Epoch: 24/500 Iteration: 290 Train loss: 0.20106956362724304\n",
      "Epoch: 24/500 Iteration: 295 Train loss: 0.11308106780052185\n",
      "Epoch: 24/500 Iteration: 300 Train loss: 0.22308020293712616\n",
      "Validation AUC: 0.5732159961685824\n",
      "Epoch: 25/500 Iteration: 305 Train loss: 0.15523050725460052\n",
      "Epoch: 25/500 Iteration: 310 Train loss: 0.19779019057750702\n",
      "Epoch: 26/500 Iteration: 315 Train loss: 0.20925426483154297\n",
      "Epoch: 26/500 Iteration: 320 Train loss: 0.1846172958612442\n",
      "Epoch: 27/500 Iteration: 325 Train loss: 0.2447700947523117\n",
      "Validation AUC: 0.5772270114942529\n",
      "Epoch: 27/500 Iteration: 330 Train loss: 0.24172386527061462\n",
      "Epoch: 27/500 Iteration: 335 Train loss: 0.2685692608356476\n",
      "Epoch: 28/500 Iteration: 340 Train loss: 0.202478289604187\n",
      "Epoch: 28/500 Iteration: 345 Train loss: 0.1798633635044098\n",
      "Epoch: 29/500 Iteration: 350 Train loss: 0.20047472417354584\n",
      "Validation AUC: 0.5818366858237548\n",
      "Epoch: 29/500 Iteration: 355 Train loss: 0.09704508632421494\n",
      "Epoch: 29/500 Iteration: 360 Train loss: 0.22633492946624756\n",
      "Epoch: 30/500 Iteration: 365 Train loss: 0.14507031440734863\n",
      "Epoch: 30/500 Iteration: 370 Train loss: 0.18325597047805786\n",
      "Epoch: 31/500 Iteration: 375 Train loss: 0.22262153029441833\n",
      "Validation AUC: 0.5856082375478927\n",
      "Epoch: 31/500 Iteration: 380 Train loss: 0.19298644363880157\n",
      "Epoch: 32/500 Iteration: 385 Train loss: 0.24487777054309845\n",
      "Epoch: 32/500 Iteration: 390 Train loss: 0.24935682117938995\n",
      "Epoch: 32/500 Iteration: 395 Train loss: 0.2755744457244873\n",
      "Epoch: 33/500 Iteration: 400 Train loss: 0.23611199855804443\n",
      "Validation AUC: 0.5921934865900383\n",
      "Epoch: 33/500 Iteration: 405 Train loss: 0.2049291729927063\n",
      "Epoch: 34/500 Iteration: 410 Train loss: 0.21980328857898712\n",
      "Epoch: 34/500 Iteration: 415 Train loss: 0.11292936652898788\n",
      "Epoch: 34/500 Iteration: 420 Train loss: 0.21293164789676666\n",
      "Epoch: 35/500 Iteration: 425 Train loss: 0.1601710468530655\n",
      "Validation AUC: 0.5978807471264368\n",
      "Epoch: 35/500 Iteration: 430 Train loss: 0.18239711225032806\n",
      "Epoch: 36/500 Iteration: 435 Train loss: 0.2173667848110199\n",
      "Epoch: 36/500 Iteration: 440 Train loss: 0.1764533668756485\n",
      "Epoch: 37/500 Iteration: 445 Train loss: 0.23557844758033752\n",
      "Epoch: 37/500 Iteration: 450 Train loss: 0.25100478529930115\n",
      "Validation AUC: 0.6033285440613027\n",
      "Epoch: 37/500 Iteration: 455 Train loss: 0.2580099403858185\n",
      "Epoch: 38/500 Iteration: 460 Train loss: 0.23416511714458466\n",
      "Epoch: 38/500 Iteration: 465 Train loss: 0.19141747057437897\n",
      "Epoch: 39/500 Iteration: 470 Train loss: 0.19796165823936462\n",
      "Epoch: 39/500 Iteration: 475 Train loss: 0.10632205754518509\n",
      "Validation AUC: 0.6086566091954023\n",
      "Epoch: 39/500 Iteration: 480 Train loss: 0.23354189097881317\n",
      "Epoch: 40/500 Iteration: 485 Train loss: 0.14446622133255005\n",
      "Epoch: 40/500 Iteration: 490 Train loss: 0.19573409855365753\n",
      "Epoch: 41/500 Iteration: 495 Train loss: 0.22204917669296265\n",
      "Epoch: 41/500 Iteration: 500 Train loss: 0.20001091063022614\n",
      "Validation AUC: 0.6148826628352491\n",
      "Epoch: 42/500 Iteration: 505 Train loss: 0.2312905341386795\n",
      "Epoch: 42/500 Iteration: 510 Train loss: 0.23546361923217773\n",
      "Epoch: 42/500 Iteration: 515 Train loss: 0.2731223404407501\n",
      "Epoch: 43/500 Iteration: 520 Train loss: 0.24816519021987915\n",
      "Epoch: 43/500 Iteration: 525 Train loss: 0.2075396329164505\n",
      "Validation AUC: 0.6220665708812261\n",
      "Epoch: 44/500 Iteration: 530 Train loss: 0.20765283703804016\n",
      "Epoch: 44/500 Iteration: 535 Train loss: 0.10291494429111481\n",
      "Epoch: 44/500 Iteration: 540 Train loss: 0.2175450176000595\n",
      "Epoch: 45/500 Iteration: 545 Train loss: 0.15188604593276978\n",
      "Epoch: 45/500 Iteration: 550 Train loss: 0.18015098571777344\n",
      "Validation AUC: 0.6300886015325671\n",
      "Epoch: 46/500 Iteration: 555 Train loss: 0.21155869960784912\n",
      "Epoch: 46/500 Iteration: 560 Train loss: 0.196819007396698\n",
      "Epoch: 47/500 Iteration: 565 Train loss: 0.22992393374443054\n",
      "Epoch: 47/500 Iteration: 570 Train loss: 0.23261567950248718\n",
      "Epoch: 47/500 Iteration: 575 Train loss: 0.2532464563846588\n",
      "Validation AUC: 0.6370929118773947\n",
      "Epoch: 48/500 Iteration: 580 Train loss: 0.22619062662124634\n",
      "Epoch: 48/500 Iteration: 585 Train loss: 0.20499053597450256\n",
      "Epoch: 49/500 Iteration: 590 Train loss: 0.20731060206890106\n",
      "Epoch: 49/500 Iteration: 595 Train loss: 0.09711180627346039\n",
      "Epoch: 49/500 Iteration: 600 Train loss: 0.22811977565288544\n",
      "Validation AUC: 0.6460129310344828\n",
      "Epoch: 50/500 Iteration: 605 Train loss: 0.14477750658988953\n",
      "Epoch: 50/500 Iteration: 610 Train loss: 0.18329447507858276\n",
      "Epoch: 51/500 Iteration: 615 Train loss: 0.21496976912021637\n",
      "Epoch: 51/500 Iteration: 620 Train loss: 0.1799936443567276\n",
      "Epoch: 52/500 Iteration: 625 Train loss: 0.22123995423316956\n",
      "Validation AUC: 0.6530172413793104\n",
      "Epoch: 52/500 Iteration: 630 Train loss: 0.248610720038414\n",
      "Epoch: 52/500 Iteration: 635 Train loss: 0.2729320526123047\n",
      "Epoch: 53/500 Iteration: 640 Train loss: 0.23627613484859467\n",
      "Epoch: 53/500 Iteration: 645 Train loss: 0.183797687292099\n",
      "Epoch: 54/500 Iteration: 650 Train loss: 0.1910904347896576\n",
      "Validation AUC: 0.6639128352490422\n",
      "Epoch: 54/500 Iteration: 655 Train loss: 0.10926678776741028\n",
      "Epoch: 54/500 Iteration: 660 Train loss: 0.2115725576877594\n",
      "Epoch: 55/500 Iteration: 665 Train loss: 0.14191888272762299\n",
      "Epoch: 55/500 Iteration: 670 Train loss: 0.17453140020370483\n",
      "Epoch: 56/500 Iteration: 675 Train loss: 0.20911294221878052\n",
      "Validation AUC: 0.670617816091954\n",
      "Epoch: 56/500 Iteration: 680 Train loss: 0.17988701164722443\n",
      "Epoch: 57/500 Iteration: 685 Train loss: 0.22200554609298706\n",
      "Epoch: 57/500 Iteration: 690 Train loss: 0.24029216170310974\n",
      "Epoch: 57/500 Iteration: 695 Train loss: 0.26580318808555603\n",
      "Epoch: 58/500 Iteration: 700 Train loss: 0.21944351494312286\n",
      "Validation AUC: 0.6797772988505748\n",
      "Epoch: 58/500 Iteration: 705 Train loss: 0.19754551351070404\n",
      "Epoch: 59/500 Iteration: 710 Train loss: 0.2058984935283661\n",
      "Epoch: 59/500 Iteration: 715 Train loss: 0.10694483667612076\n",
      "Epoch: 59/500 Iteration: 720 Train loss: 0.2185617983341217\n",
      "Epoch: 60/500 Iteration: 725 Train loss: 0.15305091440677643\n",
      "Validation AUC: 0.6863625478927203\n",
      "Epoch: 60/500 Iteration: 730 Train loss: 0.177165225148201\n",
      "Epoch: 61/500 Iteration: 735 Train loss: 0.20977061986923218\n",
      "Epoch: 61/500 Iteration: 740 Train loss: 0.1719781905412674\n",
      "Epoch: 62/500 Iteration: 745 Train loss: 0.21999605000019073\n",
      "Epoch: 62/500 Iteration: 750 Train loss: 0.24794535338878632\n",
      "Validation AUC: 0.69378591954023\n",
      "Epoch: 62/500 Iteration: 755 Train loss: 0.27761557698249817\n",
      "Epoch: 63/500 Iteration: 760 Train loss: 0.21143606305122375\n",
      "Epoch: 63/500 Iteration: 765 Train loss: 0.20005939900875092\n",
      "Epoch: 64/500 Iteration: 770 Train loss: 0.2211284190416336\n",
      "Epoch: 64/500 Iteration: 775 Train loss: 0.11231520771980286\n",
      "Validation AUC: 0.7021671455938696\n",
      "Epoch: 64/500 Iteration: 780 Train loss: 0.2039686143398285\n",
      "Epoch: 65/500 Iteration: 785 Train loss: 0.1379420906305313\n",
      "Epoch: 65/500 Iteration: 790 Train loss: 0.16529397666454315\n",
      "Epoch: 66/500 Iteration: 795 Train loss: 0.21859939396381378\n",
      "Epoch: 66/500 Iteration: 800 Train loss: 0.16851621866226196\n",
      "Validation AUC: 0.7041427203065134\n",
      "Epoch: 67/500 Iteration: 805 Train loss: 0.2212168574333191\n",
      "Epoch: 67/500 Iteration: 810 Train loss: 0.21512949466705322\n",
      "Epoch: 67/500 Iteration: 815 Train loss: 0.2623521387577057\n",
      "Epoch: 68/500 Iteration: 820 Train loss: 0.22367849946022034\n",
      "Epoch: 68/500 Iteration: 825 Train loss: 0.20833951234817505\n",
      "Validation AUC: 0.704382183908046\n",
      "Epoch: 69/500 Iteration: 830 Train loss: 0.21767602860927582\n",
      "Epoch: 69/500 Iteration: 835 Train loss: 0.10966215282678604\n",
      "Epoch: 69/500 Iteration: 840 Train loss: 0.21078865230083466\n",
      "Epoch: 70/500 Iteration: 845 Train loss: 0.12922535836696625\n",
      "Epoch: 70/500 Iteration: 850 Train loss: 0.18872225284576416\n",
      "Validation AUC: 0.7068366858237548\n",
      "Epoch: 71/500 Iteration: 855 Train loss: 0.22162635624408722\n",
      "Epoch: 71/500 Iteration: 860 Train loss: 0.18539661169052124\n",
      "Epoch: 72/500 Iteration: 865 Train loss: 0.218050017952919\n",
      "Epoch: 72/500 Iteration: 870 Train loss: 0.24249428510665894\n",
      "Epoch: 72/500 Iteration: 875 Train loss: 0.24981200695037842\n",
      "Validation AUC: 0.7089319923371648\n",
      "Epoch: 73/500 Iteration: 880 Train loss: 0.20568914711475372\n",
      "Epoch: 73/500 Iteration: 885 Train loss: 0.19038981199264526\n",
      "Epoch: 74/500 Iteration: 890 Train loss: 0.2005900740623474\n",
      "Epoch: 74/500 Iteration: 895 Train loss: 0.10756093263626099\n",
      "Epoch: 74/500 Iteration: 900 Train loss: 0.20251689851284027\n",
      "Validation AUC: 0.7056393678160919\n",
      "Epoch: 75/500 Iteration: 905 Train loss: 0.12883500754833221\n",
      "Epoch: 75/500 Iteration: 910 Train loss: 0.1589108109474182\n",
      "Epoch: 76/500 Iteration: 915 Train loss: 0.20364032685756683\n",
      "Epoch: 76/500 Iteration: 920 Train loss: 0.17536531388759613\n",
      "Epoch: 77/500 Iteration: 925 Train loss: 0.22141441702842712\n",
      "Validation AUC: 0.70522030651341\n",
      "Epoch: 77/500 Iteration: 930 Train loss: 0.23896664381027222\n",
      "Epoch: 77/500 Iteration: 935 Train loss: 0.2611846327781677\n",
      "Epoch: 78/500 Iteration: 940 Train loss: 0.20979471504688263\n",
      "Epoch: 78/500 Iteration: 945 Train loss: 0.20423954725265503\n",
      "Epoch: 79/500 Iteration: 950 Train loss: 0.2082667350769043\n",
      "Validation AUC: 0.703544061302682\n",
      "Epoch: 79/500 Iteration: 955 Train loss: 0.10499447584152222\n",
      "Epoch: 79/500 Iteration: 960 Train loss: 0.2072787880897522\n",
      "Epoch: 80/500 Iteration: 965 Train loss: 0.13457760214805603\n",
      "Epoch: 80/500 Iteration: 970 Train loss: 0.16945227980613708\n",
      "Epoch: 81/500 Iteration: 975 Train loss: 0.2011021375656128\n",
      "Validation AUC: 0.6979166666666666\n",
      "Epoch: 81/500 Iteration: 980 Train loss: 0.16942240297794342\n",
      "Epoch: 82/500 Iteration: 985 Train loss: 0.2325010448694229\n",
      "Epoch: 82/500 Iteration: 990 Train loss: 0.24203749001026154\n",
      "Epoch: 82/500 Iteration: 995 Train loss: 0.24907870590686798\n",
      "Epoch: 83/500 Iteration: 1000 Train loss: 0.20493656396865845\n",
      "Validation AUC: 0.6898946360153257\n",
      "Epoch: 83/500 Iteration: 1005 Train loss: 0.1699143797159195\n",
      "Epoch: 84/500 Iteration: 1010 Train loss: 0.19595077633857727\n",
      "Epoch: 84/500 Iteration: 1015 Train loss: 0.09020590037107468\n",
      "Epoch: 84/500 Iteration: 1020 Train loss: 0.21712130308151245\n",
      "Epoch: 85/500 Iteration: 1025 Train loss: 0.13060607016086578\n",
      "Validation AUC: 0.683249521072797\n",
      "Epoch: 85/500 Iteration: 1030 Train loss: 0.18311019241809845\n",
      "Epoch: 86/500 Iteration: 1035 Train loss: 0.19263631105422974\n",
      "Epoch: 86/500 Iteration: 1040 Train loss: 0.14678730070590973\n",
      "Epoch: 87/500 Iteration: 1045 Train loss: 0.20550747215747833\n",
      "Epoch: 87/500 Iteration: 1050 Train loss: 0.2329825907945633\n",
      "Validation AUC: 0.6724137931034483\n",
      "Epoch: 87/500 Iteration: 1055 Train loss: 0.2526143789291382\n",
      "Epoch: 88/500 Iteration: 1060 Train loss: 0.1896320879459381\n",
      "Epoch: 88/500 Iteration: 1065 Train loss: 0.172671839594841\n",
      "Epoch: 89/500 Iteration: 1070 Train loss: 0.20281915366649628\n",
      "Epoch: 89/500 Iteration: 1075 Train loss: 0.09764163941144943\n",
      "Validation AUC: 0.6649904214559388\n",
      "Epoch: 89/500 Iteration: 1080 Train loss: 0.21661561727523804\n",
      "Epoch: 90/500 Iteration: 1085 Train loss: 0.11636357009410858\n",
      "Epoch: 90/500 Iteration: 1090 Train loss: 0.17255939543247223\n",
      "Epoch: 91/500 Iteration: 1095 Train loss: 0.19118916988372803\n",
      "Epoch: 91/500 Iteration: 1100 Train loss: 0.1679151952266693\n",
      "Validation AUC: 0.6527179118773947\n",
      "Epoch: 92/500 Iteration: 1105 Train loss: 0.21492509543895721\n",
      "Epoch: 92/500 Iteration: 1110 Train loss: 0.2167421281337738\n",
      "Epoch: 92/500 Iteration: 1115 Train loss: 0.251071959733963\n",
      "Epoch: 93/500 Iteration: 1120 Train loss: 0.2025957852602005\n",
      "Epoch: 93/500 Iteration: 1125 Train loss: 0.18435275554656982\n",
      "Validation AUC: 0.6426604406130269\n",
      "Epoch: 94/500 Iteration: 1130 Train loss: 0.1929459571838379\n",
      "Epoch: 94/500 Iteration: 1135 Train loss: 0.09588487446308136\n",
      "Epoch: 94/500 Iteration: 1140 Train loss: 0.184926837682724\n",
      "Epoch: 95/500 Iteration: 1145 Train loss: 0.10899901390075684\n",
      "Epoch: 95/500 Iteration: 1150 Train loss: 0.15617194771766663\n",
      "Validation AUC: 0.641882183908046\n",
      "Epoch: 96/500 Iteration: 1155 Train loss: 0.1951027810573578\n",
      "Epoch: 96/500 Iteration: 1160 Train loss: 0.14154252409934998\n",
      "Epoch: 97/500 Iteration: 1165 Train loss: 0.2057691216468811\n",
      "Epoch: 97/500 Iteration: 1170 Train loss: 0.20952770113945007\n",
      "Epoch: 97/500 Iteration: 1175 Train loss: 0.258727490901947\n",
      "Validation AUC: 0.6363146551724138\n",
      "Epoch: 98/500 Iteration: 1180 Train loss: 0.1744323968887329\n",
      "Epoch: 98/500 Iteration: 1185 Train loss: 0.16451388597488403\n",
      "Epoch: 99/500 Iteration: 1190 Train loss: 0.19680048525333405\n",
      "Epoch: 99/500 Iteration: 1195 Train loss: 0.0951879471540451\n",
      "Epoch: 99/500 Iteration: 1200 Train loss: 0.2120702862739563\n",
      "Validation AUC: 0.6294300766283525\n",
      "Epoch: 100/500 Iteration: 1205 Train loss: 0.11138493567705154\n",
      "Epoch: 100/500 Iteration: 1210 Train loss: 0.1462155282497406\n",
      "Epoch: 101/500 Iteration: 1215 Train loss: 0.17965546250343323\n",
      "Epoch: 101/500 Iteration: 1220 Train loss: 0.13136038184165955\n",
      "Epoch: 102/500 Iteration: 1225 Train loss: 0.19810569286346436\n",
      "Validation AUC: 0.6303280651340997\n",
      "Epoch: 102/500 Iteration: 1230 Train loss: 0.22426961362361908\n",
      "Epoch: 102/500 Iteration: 1235 Train loss: 0.23997297883033752\n",
      "Epoch: 103/500 Iteration: 1240 Train loss: 0.17530697584152222\n",
      "Epoch: 103/500 Iteration: 1245 Train loss: 0.19453300535678864\n",
      "Epoch: 104/500 Iteration: 1250 Train loss: 0.19763150811195374\n",
      "Validation AUC: 0.6226053639846743\n",
      "Epoch: 104/500 Iteration: 1255 Train loss: 0.08483883738517761\n",
      "Epoch: 104/500 Iteration: 1260 Train loss: 0.17978888750076294\n",
      "Epoch: 105/500 Iteration: 1265 Train loss: 0.10586875677108765\n",
      "Epoch: 105/500 Iteration: 1270 Train loss: 0.15572871267795563\n",
      "Epoch: 106/500 Iteration: 1275 Train loss: 0.1865469366312027\n",
      "Validation AUC: 0.6261374521072797\n",
      "Epoch: 106/500 Iteration: 1280 Train loss: 0.13102994859218597\n",
      "Epoch: 107/500 Iteration: 1285 Train loss: 0.19038227200508118\n",
      "Epoch: 107/500 Iteration: 1290 Train loss: 0.19085238873958588\n",
      "Epoch: 107/500 Iteration: 1295 Train loss: 0.24483011662960052\n",
      "Epoch: 108/500 Iteration: 1300 Train loss: 0.18154172599315643\n",
      "Validation AUC: 0.6272150383141762\n",
      "Epoch: 108/500 Iteration: 1305 Train loss: 0.1725873202085495\n",
      "Epoch: 109/500 Iteration: 1310 Train loss: 0.19199182093143463\n",
      "Epoch: 109/500 Iteration: 1315 Train loss: 0.09395138919353485\n",
      "Epoch: 109/500 Iteration: 1320 Train loss: 0.20823566615581512\n",
      "Epoch: 110/500 Iteration: 1325 Train loss: 0.1153680831193924\n",
      "Validation AUC: 0.6288912835249043\n",
      "Epoch: 110/500 Iteration: 1330 Train loss: 0.1452338844537735\n",
      "Epoch: 111/500 Iteration: 1335 Train loss: 0.17917761206626892\n",
      "Epoch: 111/500 Iteration: 1340 Train loss: 0.13664460182189941\n",
      "Epoch: 112/500 Iteration: 1345 Train loss: 0.17622019350528717\n",
      "Epoch: 112/500 Iteration: 1350 Train loss: 0.1948360949754715\n",
      "Validation AUC: 0.62709530651341\n",
      "Epoch: 112/500 Iteration: 1355 Train loss: 0.2430611401796341\n",
      "Epoch: 113/500 Iteration: 1360 Train loss: 0.15630489587783813\n",
      "Epoch: 113/500 Iteration: 1365 Train loss: 0.16784238815307617\n",
      "Epoch: 114/500 Iteration: 1370 Train loss: 0.18187527358531952\n",
      "Epoch: 114/500 Iteration: 1375 Train loss: 0.08826371282339096\n",
      "Validation AUC: 0.634279214559387\n",
      "Epoch: 114/500 Iteration: 1380 Train loss: 0.18879713118076324\n",
      "Epoch: 115/500 Iteration: 1385 Train loss: 0.11925005912780762\n",
      "Epoch: 115/500 Iteration: 1390 Train loss: 0.14024578034877777\n",
      "Epoch: 116/500 Iteration: 1395 Train loss: 0.18649645149707794\n",
      "Epoch: 116/500 Iteration: 1400 Train loss: 0.14115294814109802\n",
      "Validation AUC: 0.6380507662835249\n",
      "Epoch: 117/500 Iteration: 1405 Train loss: 0.19394345581531525\n",
      "Epoch: 117/500 Iteration: 1410 Train loss: 0.19566930830478668\n",
      "Epoch: 117/500 Iteration: 1415 Train loss: 0.24675525724887848\n",
      "Epoch: 118/500 Iteration: 1420 Train loss: 0.16444289684295654\n",
      "Epoch: 118/500 Iteration: 1425 Train loss: 0.17743512988090515\n",
      "Validation AUC: 0.6452346743295019\n",
      "Epoch: 119/500 Iteration: 1430 Train loss: 0.16523611545562744\n",
      "Epoch: 119/500 Iteration: 1435 Train loss: 0.08045752346515656\n",
      "Epoch: 119/500 Iteration: 1440 Train loss: 0.19019781053066254\n",
      "Epoch: 120/500 Iteration: 1445 Train loss: 0.10651897639036179\n",
      "Epoch: 120/500 Iteration: 1450 Train loss: 0.14725972712039948\n",
      "Validation AUC: 0.6425407088122606\n",
      "Epoch: 121/500 Iteration: 1455 Train loss: 0.1870303750038147\n",
      "Epoch: 121/500 Iteration: 1460 Train loss: 0.1165260598063469\n",
      "Epoch: 122/500 Iteration: 1465 Train loss: 0.19996370375156403\n",
      "Epoch: 122/500 Iteration: 1470 Train loss: 0.19483445584774017\n",
      "Epoch: 122/500 Iteration: 1475 Train loss: 0.2490237057209015\n",
      "Validation AUC: 0.6379909003831418\n",
      "Epoch: 123/500 Iteration: 1480 Train loss: 0.17157219350337982\n",
      "Epoch: 123/500 Iteration: 1485 Train loss: 0.16112424433231354\n",
      "Epoch: 124/500 Iteration: 1490 Train loss: 0.16822612285614014\n",
      "Epoch: 124/500 Iteration: 1495 Train loss: 0.07381244003772736\n",
      "Epoch: 124/500 Iteration: 1500 Train loss: 0.1863734871149063\n",
      "Validation AUC: 0.6417624521072797\n",
      "Epoch: 125/500 Iteration: 1505 Train loss: 0.09854569286108017\n",
      "Epoch: 125/500 Iteration: 1510 Train loss: 0.14489135146141052\n",
      "Epoch: 126/500 Iteration: 1515 Train loss: 0.1770201474428177\n",
      "Epoch: 126/500 Iteration: 1520 Train loss: 0.12391399592161179\n",
      "Epoch: 127/500 Iteration: 1525 Train loss: 0.18198949098587036\n",
      "Validation AUC: 0.6466714559386972\n",
      "Epoch: 127/500 Iteration: 1530 Train loss: 0.19585847854614258\n",
      "Epoch: 127/500 Iteration: 1535 Train loss: 0.2508012354373932\n",
      "Epoch: 128/500 Iteration: 1540 Train loss: 0.1579747051000595\n",
      "Epoch: 128/500 Iteration: 1545 Train loss: 0.16107912361621857\n",
      "Epoch: 129/500 Iteration: 1550 Train loss: 0.1597232222557068\n",
      "Validation AUC: 0.6482279693486589\n",
      "Epoch: 129/500 Iteration: 1555 Train loss: 0.07595673203468323\n",
      "Epoch: 129/500 Iteration: 1560 Train loss: 0.1820312738418579\n",
      "Epoch: 130/500 Iteration: 1565 Train loss: 0.1036253347992897\n",
      "Epoch: 130/500 Iteration: 1570 Train loss: 0.15336379408836365\n",
      "Epoch: 131/500 Iteration: 1575 Train loss: 0.1773722767829895\n",
      "Validation AUC: 0.6506824712643678\n",
      "Epoch: 131/500 Iteration: 1580 Train loss: 0.11981966346502304\n",
      "Epoch: 132/500 Iteration: 1585 Train loss: 0.18538492918014526\n",
      "Epoch: 132/500 Iteration: 1590 Train loss: 0.18631629645824432\n",
      "Epoch: 132/500 Iteration: 1595 Train loss: 0.24762849509716034\n",
      "Epoch: 133/500 Iteration: 1600 Train loss: 0.1481940895318985\n",
      "Validation AUC: 0.6464319923371646\n",
      "Epoch: 133/500 Iteration: 1605 Train loss: 0.16420400142669678\n",
      "Epoch: 134/500 Iteration: 1610 Train loss: 0.1589585542678833\n",
      "Epoch: 134/500 Iteration: 1615 Train loss: 0.07120100408792496\n",
      "Epoch: 134/500 Iteration: 1620 Train loss: 0.17820888757705688\n",
      "Epoch: 135/500 Iteration: 1625 Train loss: 0.09830917418003082\n",
      "Validation AUC: 0.6522389846743295\n",
      "Epoch: 135/500 Iteration: 1630 Train loss: 0.14911600947380066\n",
      "Epoch: 136/500 Iteration: 1635 Train loss: 0.16396358609199524\n",
      "Epoch: 136/500 Iteration: 1640 Train loss: 0.12087131291627884\n",
      "Epoch: 137/500 Iteration: 1645 Train loss: 0.1779100000858307\n",
      "Epoch: 137/500 Iteration: 1650 Train loss: 0.19015781581401825\n",
      "Validation AUC: 0.6521791187739464\n",
      "Epoch: 137/500 Iteration: 1655 Train loss: 0.25001275539398193\n",
      "Epoch: 138/500 Iteration: 1660 Train loss: 0.16334480047225952\n",
      "Epoch: 138/500 Iteration: 1665 Train loss: 0.14144252240657806\n",
      "Epoch: 139/500 Iteration: 1670 Train loss: 0.14068090915679932\n",
      "Epoch: 139/500 Iteration: 1675 Train loss: 0.07391980290412903\n",
      "Validation AUC: 0.6552322796934866\n",
      "Epoch: 139/500 Iteration: 1680 Train loss: 0.17707176506519318\n",
      "Epoch: 140/500 Iteration: 1685 Train loss: 0.08800967037677765\n",
      "Epoch: 140/500 Iteration: 1690 Train loss: 0.12083049863576889\n",
      "Epoch: 141/500 Iteration: 1695 Train loss: 0.15997815132141113\n",
      "Epoch: 141/500 Iteration: 1700 Train loss: 0.11082445830106735\n",
      "Validation AUC: 0.656669061302682\n",
      "Epoch: 142/500 Iteration: 1705 Train loss: 0.16912952065467834\n",
      "Epoch: 142/500 Iteration: 1710 Train loss: 0.18620286881923676\n",
      "Epoch: 142/500 Iteration: 1715 Train loss: 0.23138923943042755\n",
      "Epoch: 143/500 Iteration: 1720 Train loss: 0.1330617219209671\n",
      "Epoch: 143/500 Iteration: 1725 Train loss: 0.17111705243587494\n",
      "Validation AUC: 0.654573754789272\n",
      "Epoch: 144/500 Iteration: 1730 Train loss: 0.16272428631782532\n",
      "Epoch: 144/500 Iteration: 1735 Train loss: 0.06977525353431702\n",
      "Epoch: 144/500 Iteration: 1740 Train loss: 0.16931545734405518\n",
      "Epoch: 145/500 Iteration: 1745 Train loss: 0.09053771942853928\n",
      "Epoch: 145/500 Iteration: 1750 Train loss: 0.125276118516922\n",
      "Validation AUC: 0.6591834291187739\n",
      "Epoch: 146/500 Iteration: 1755 Train loss: 0.1384674608707428\n",
      "Epoch: 146/500 Iteration: 1760 Train loss: 0.09621907025575638\n",
      "Epoch: 147/500 Iteration: 1765 Train loss: 0.18830375373363495\n",
      "Epoch: 147/500 Iteration: 1770 Train loss: 0.18708151578903198\n",
      "Epoch: 147/500 Iteration: 1775 Train loss: 0.22594085335731506\n",
      "Validation AUC: 0.6600814176245211\n",
      "Epoch: 148/500 Iteration: 1780 Train loss: 0.13114146888256073\n",
      "Epoch: 148/500 Iteration: 1785 Train loss: 0.14477616548538208\n",
      "Epoch: 149/500 Iteration: 1790 Train loss: 0.14710134267807007\n",
      "Epoch: 149/500 Iteration: 1795 Train loss: 0.06217912584543228\n",
      "Epoch: 149/500 Iteration: 1800 Train loss: 0.17863669991493225\n",
      "Validation AUC: 0.6657686781609196\n",
      "Epoch: 150/500 Iteration: 1805 Train loss: 0.07988931983709335\n",
      "Epoch: 150/500 Iteration: 1810 Train loss: 0.1547296792268753\n",
      "Epoch: 151/500 Iteration: 1815 Train loss: 0.15077991783618927\n",
      "Epoch: 151/500 Iteration: 1820 Train loss: 0.1067323386669159\n",
      "Epoch: 152/500 Iteration: 1825 Train loss: 0.17953330278396606\n",
      "Validation AUC: 0.6451149425287357\n",
      "Epoch: 152/500 Iteration: 1830 Train loss: 0.15360455214977264\n",
      "Epoch: 152/500 Iteration: 1835 Train loss: 0.23161008954048157\n",
      "Epoch: 153/500 Iteration: 1840 Train loss: 0.13095492124557495\n",
      "Epoch: 153/500 Iteration: 1845 Train loss: 0.13064195215702057\n",
      "Epoch: 154/500 Iteration: 1850 Train loss: 0.15994490683078766\n",
      "Validation AUC: 0.6522389846743294\n",
      "Epoch: 154/500 Iteration: 1855 Train loss: 0.06202465668320656\n",
      "Epoch: 154/500 Iteration: 1860 Train loss: 0.16218942403793335\n",
      "Epoch: 155/500 Iteration: 1865 Train loss: 0.0864872857928276\n",
      "Epoch: 155/500 Iteration: 1870 Train loss: 0.14939019083976746\n",
      "Epoch: 156/500 Iteration: 1875 Train loss: 0.1516030728816986\n",
      "Validation AUC: 0.6592432950191571\n",
      "Epoch: 156/500 Iteration: 1880 Train loss: 0.09871653467416763\n",
      "Epoch: 157/500 Iteration: 1885 Train loss: 0.17276804149150848\n",
      "Epoch: 157/500 Iteration: 1890 Train loss: 0.16557759046554565\n",
      "Epoch: 157/500 Iteration: 1895 Train loss: 0.24836401641368866\n",
      "Epoch: 158/500 Iteration: 1900 Train loss: 0.1407693326473236\n",
      "Validation AUC: 0.657507183908046\n",
      "Epoch: 158/500 Iteration: 1905 Train loss: 0.13338826596736908\n",
      "Epoch: 159/500 Iteration: 1910 Train loss: 0.15033291280269623\n",
      "Epoch: 159/500 Iteration: 1915 Train loss: 0.05948337912559509\n",
      "Epoch: 159/500 Iteration: 1920 Train loss: 0.1465088278055191\n",
      "Epoch: 160/500 Iteration: 1925 Train loss: 0.07971235364675522\n",
      "Validation AUC: 0.6527777777777778\n",
      "Epoch: 160/500 Iteration: 1930 Train loss: 0.13011951744556427\n",
      "Epoch: 161/500 Iteration: 1935 Train loss: 0.13742008805274963\n",
      "Epoch: 161/500 Iteration: 1940 Train loss: 0.09944070875644684\n",
      "Epoch: 162/500 Iteration: 1945 Train loss: 0.18850606679916382\n",
      "Epoch: 162/500 Iteration: 1950 Train loss: 0.14929012954235077\n",
      "Validation AUC: 0.6551125478927202\n",
      "Epoch: 162/500 Iteration: 1955 Train loss: 0.23790906369686127\n",
      "Epoch: 163/500 Iteration: 1960 Train loss: 0.12232886254787445\n",
      "Epoch: 163/500 Iteration: 1965 Train loss: 0.1403607875108719\n",
      "Epoch: 164/500 Iteration: 1970 Train loss: 0.14376328885555267\n",
      "Epoch: 164/500 Iteration: 1975 Train loss: 0.06403876096010208\n",
      "Validation AUC: 0.6630148467432949\n",
      "Epoch: 164/500 Iteration: 1980 Train loss: 0.15493398904800415\n",
      "Epoch: 165/500 Iteration: 1985 Train loss: 0.07754503190517426\n",
      "Epoch: 165/500 Iteration: 1990 Train loss: 0.12033122032880783\n",
      "Epoch: 166/500 Iteration: 1995 Train loss: 0.11777715384960175\n",
      "Epoch: 166/500 Iteration: 2000 Train loss: 0.1045231819152832\n",
      "Validation AUC: 0.654094827586207\n",
      "Epoch: 167/500 Iteration: 2005 Train loss: 0.16371017694473267\n",
      "Epoch: 167/500 Iteration: 2010 Train loss: 0.13867825269699097\n",
      "Epoch: 167/500 Iteration: 2015 Train loss: 0.2427055686712265\n",
      "Epoch: 168/500 Iteration: 2020 Train loss: 0.1346241533756256\n",
      "Epoch: 168/500 Iteration: 2025 Train loss: 0.13192497193813324\n",
      "Validation AUC: 0.6581657088122604\n",
      "Epoch: 169/500 Iteration: 2030 Train loss: 0.15120568871498108\n",
      "Epoch: 169/500 Iteration: 2035 Train loss: 0.057605043053627014\n",
      "Epoch: 169/500 Iteration: 2040 Train loss: 0.15517060458660126\n",
      "Epoch: 170/500 Iteration: 2045 Train loss: 0.08638069033622742\n",
      "Epoch: 170/500 Iteration: 2050 Train loss: 0.12169047445058823\n",
      "Validation AUC: 0.6368534482758621\n",
      "Epoch: 171/500 Iteration: 2055 Train loss: 0.1483219861984253\n",
      "Epoch: 171/500 Iteration: 2060 Train loss: 0.11126582324504852\n",
      "Epoch: 172/500 Iteration: 2065 Train loss: 0.17856411635875702\n",
      "Epoch: 172/500 Iteration: 2070 Train loss: 0.1564246118068695\n",
      "Epoch: 172/500 Iteration: 2075 Train loss: 0.2648455500602722\n",
      "Validation AUC: 0.6417624521072797\n",
      "Epoch: 173/500 Iteration: 2080 Train loss: 0.12737709283828735\n",
      "Epoch: 173/500 Iteration: 2085 Train loss: 0.14252512156963348\n",
      "Epoch: 174/500 Iteration: 2090 Train loss: 0.1527467519044876\n",
      "Epoch: 174/500 Iteration: 2095 Train loss: 0.051610950380563736\n",
      "Epoch: 174/500 Iteration: 2100 Train loss: 0.15428294241428375\n",
      "Validation AUC: 0.6557710727969348\n",
      "Epoch: 175/500 Iteration: 2105 Train loss: 0.07316490262746811\n",
      "Epoch: 175/500 Iteration: 2110 Train loss: 0.12268145382404327\n",
      "Epoch: 176/500 Iteration: 2115 Train loss: 0.12104771286249161\n",
      "Epoch: 176/500 Iteration: 2120 Train loss: 0.0965537279844284\n",
      "Epoch: 177/500 Iteration: 2125 Train loss: 0.1702837496995926\n",
      "Validation AUC: 0.6492456896551724\n",
      "Epoch: 177/500 Iteration: 2130 Train loss: 0.12229973822832108\n",
      "Epoch: 177/500 Iteration: 2135 Train loss: 0.24059690535068512\n",
      "Epoch: 178/500 Iteration: 2140 Train loss: 0.13248533010482788\n",
      "Epoch: 178/500 Iteration: 2145 Train loss: 0.13498686254024506\n",
      "Epoch: 179/500 Iteration: 2150 Train loss: 0.15256290137767792\n",
      "Validation AUC: 0.6603807471264367\n",
      "Epoch: 179/500 Iteration: 2155 Train loss: 0.04797561094164848\n",
      "Epoch: 179/500 Iteration: 2160 Train loss: 0.15258647501468658\n",
      "Epoch: 180/500 Iteration: 2165 Train loss: 0.05901142209768295\n",
      "Epoch: 180/500 Iteration: 2170 Train loss: 0.13041365146636963\n",
      "Epoch: 181/500 Iteration: 2175 Train loss: 0.09432829916477203\n",
      "Validation AUC: 0.6602610153256705\n",
      "Epoch: 181/500 Iteration: 2180 Train loss: 0.10527171194553375\n",
      "Epoch: 182/500 Iteration: 2185 Train loss: 0.16596516966819763\n",
      "Epoch: 182/500 Iteration: 2190 Train loss: 0.10878930240869522\n",
      "Epoch: 182/500 Iteration: 2195 Train loss: 0.23467233777046204\n",
      "Epoch: 183/500 Iteration: 2200 Train loss: 0.1354830414056778\n",
      "Validation AUC: 0.6698694923371648\n",
      "Epoch: 183/500 Iteration: 2205 Train loss: 0.1141657754778862\n",
      "Epoch: 184/500 Iteration: 2210 Train loss: 0.1354181170463562\n",
      "Epoch: 184/500 Iteration: 2215 Train loss: 0.05005598068237305\n",
      "Epoch: 184/500 Iteration: 2220 Train loss: 0.14672988653182983\n",
      "Epoch: 185/500 Iteration: 2225 Train loss: 0.06681669503450394\n",
      "Validation AUC: 0.6563098659003832\n",
      "Epoch: 185/500 Iteration: 2230 Train loss: 0.10883553326129913\n",
      "Epoch: 186/500 Iteration: 2235 Train loss: 0.10788724571466446\n",
      "Epoch: 186/500 Iteration: 2240 Train loss: 0.0969085767865181\n",
      "Epoch: 187/500 Iteration: 2245 Train loss: 0.1739576905965805\n",
      "Epoch: 187/500 Iteration: 2250 Train loss: 0.11405639350414276\n",
      "Validation AUC: 0.6681633141762452\n",
      "Epoch: 187/500 Iteration: 2255 Train loss: 0.2434089034795761\n",
      "Epoch: 188/500 Iteration: 2260 Train loss: 0.127396360039711\n",
      "Epoch: 188/500 Iteration: 2265 Train loss: 0.19809094071388245\n",
      "Epoch: 189/500 Iteration: 2270 Train loss: 0.1679697185754776\n",
      "Epoch: 189/500 Iteration: 2275 Train loss: 0.09716406464576721\n",
      "Validation AUC: 0.6442169540229885\n",
      "Epoch: 189/500 Iteration: 2280 Train loss: 0.19693498313426971\n",
      "Epoch: 190/500 Iteration: 2285 Train loss: 0.07531823962926865\n",
      "Epoch: 190/500 Iteration: 2290 Train loss: 0.16103968024253845\n",
      "Epoch: 191/500 Iteration: 2295 Train loss: 0.11679757386445999\n",
      "Epoch: 191/500 Iteration: 2300 Train loss: 0.09203429520130157\n",
      "Validation AUC: 0.6686422413793103\n",
      "Epoch: 192/500 Iteration: 2305 Train loss: 0.19628767669200897\n",
      "Epoch: 192/500 Iteration: 2310 Train loss: 0.13961611688137054\n",
      "Epoch: 192/500 Iteration: 2315 Train loss: 0.2462640255689621\n",
      "Epoch: 193/500 Iteration: 2320 Train loss: 0.13655325770378113\n",
      "Epoch: 193/500 Iteration: 2325 Train loss: 0.13944502174854279\n",
      "Validation AUC: 0.673132183908046\n",
      "Epoch: 194/500 Iteration: 2330 Train loss: 0.13594798743724823\n",
      "Epoch: 194/500 Iteration: 2335 Train loss: 0.06891561299562454\n",
      "Epoch: 194/500 Iteration: 2340 Train loss: 0.13800577819347382\n",
      "Epoch: 195/500 Iteration: 2345 Train loss: 0.063722163438797\n",
      "Epoch: 195/500 Iteration: 2350 Train loss: 0.13764843344688416\n",
      "Validation AUC: 0.6764248084291188\n",
      "Epoch: 196/500 Iteration: 2355 Train loss: 0.10312292724847794\n",
      "Epoch: 196/500 Iteration: 2360 Train loss: 0.08976840227842331\n",
      "Epoch: 197/500 Iteration: 2365 Train loss: 0.15045392513275146\n",
      "Epoch: 197/500 Iteration: 2370 Train loss: 0.1306726485490799\n",
      "Epoch: 197/500 Iteration: 2375 Train loss: 0.23327982425689697\n",
      "Validation AUC: 0.6787595785440613\n",
      "Epoch: 198/500 Iteration: 2380 Train loss: 0.12151136994361877\n",
      "Epoch: 198/500 Iteration: 2385 Train loss: 0.1403154730796814\n",
      "Epoch: 199/500 Iteration: 2390 Train loss: 0.14173366129398346\n",
      "Epoch: 199/500 Iteration: 2395 Train loss: 0.04694109037518501\n",
      "Epoch: 199/500 Iteration: 2400 Train loss: 0.14370496571063995\n",
      "Validation AUC: 0.6762452107279694\n",
      "Epoch: 200/500 Iteration: 2405 Train loss: 0.06467239558696747\n",
      "Epoch: 200/500 Iteration: 2410 Train loss: 0.12193090468645096\n",
      "Epoch: 201/500 Iteration: 2415 Train loss: 0.10728530585765839\n",
      "Epoch: 201/500 Iteration: 2420 Train loss: 0.09153509885072708\n",
      "Epoch: 202/500 Iteration: 2425 Train loss: 0.15059249103069305\n",
      "Validation AUC: 0.6704382183908046\n",
      "Epoch: 202/500 Iteration: 2430 Train loss: 0.11928796023130417\n",
      "Epoch: 202/500 Iteration: 2435 Train loss: 0.22696541249752045\n",
      "Epoch: 203/500 Iteration: 2440 Train loss: 0.1327929049730301\n",
      "Epoch: 203/500 Iteration: 2445 Train loss: 0.11521673947572708\n",
      "Epoch: 204/500 Iteration: 2450 Train loss: 0.13526618480682373\n",
      "Validation AUC: 0.6721743295019157\n",
      "Epoch: 204/500 Iteration: 2455 Train loss: 0.04788517579436302\n",
      "Epoch: 204/500 Iteration: 2460 Train loss: 0.13833960890769958\n",
      "Epoch: 205/500 Iteration: 2465 Train loss: 0.06310810148715973\n",
      "Epoch: 205/500 Iteration: 2470 Train loss: 0.11895155906677246\n",
      "Epoch: 206/500 Iteration: 2475 Train loss: 0.0960666686296463\n",
      "Validation AUC: 0.669300766283525\n",
      "Epoch: 206/500 Iteration: 2480 Train loss: 0.09114935249090195\n",
      "Epoch: 207/500 Iteration: 2485 Train loss: 0.1732800453901291\n",
      "Epoch: 207/500 Iteration: 2490 Train loss: 0.1311509758234024\n",
      "Epoch: 207/500 Iteration: 2495 Train loss: 0.22346660494804382\n",
      "Epoch: 208/500 Iteration: 2500 Train loss: 0.10851845890283585\n",
      "Validation AUC: 0.6723539272030651\n",
      "Epoch: 208/500 Iteration: 2505 Train loss: 0.12216714769601822\n",
      "Epoch: 209/500 Iteration: 2510 Train loss: 0.12635061144828796\n",
      "Epoch: 209/500 Iteration: 2515 Train loss: 0.053628936409950256\n",
      "Epoch: 209/500 Iteration: 2520 Train loss: 0.14705590903759003\n",
      "Epoch: 210/500 Iteration: 2525 Train loss: 0.05642138049006462\n",
      "Validation AUC: 0.6722341954022989\n",
      "Epoch: 210/500 Iteration: 2530 Train loss: 0.10797320306301117\n",
      "Epoch: 211/500 Iteration: 2535 Train loss: 0.09043683856725693\n",
      "Epoch: 211/500 Iteration: 2540 Train loss: 0.07115953415632248\n",
      "Epoch: 212/500 Iteration: 2545 Train loss: 0.17287510633468628\n",
      "Epoch: 212/500 Iteration: 2550 Train loss: 0.11797616630792618\n",
      "Validation AUC: 0.6721743295019158\n",
      "Epoch: 212/500 Iteration: 2555 Train loss: 0.22429995238780975\n",
      "Epoch: 213/500 Iteration: 2560 Train loss: 0.12030111253261566\n",
      "Epoch: 213/500 Iteration: 2565 Train loss: 0.11492433398962021\n",
      "Epoch: 214/500 Iteration: 2570 Train loss: 0.12547431886196136\n",
      "Epoch: 214/500 Iteration: 2575 Train loss: 0.04523793235421181\n",
      "Validation AUC: 0.6741499042145594\n",
      "Epoch: 214/500 Iteration: 2580 Train loss: 0.1478690803050995\n",
      "Epoch: 215/500 Iteration: 2585 Train loss: 0.057485416531562805\n",
      "Epoch: 215/500 Iteration: 2590 Train loss: 0.12630969285964966\n",
      "Epoch: 216/500 Iteration: 2595 Train loss: 0.10223512351512909\n",
      "Epoch: 216/500 Iteration: 2600 Train loss: 0.07266273349523544\n",
      "Validation AUC: 0.6689415708812261\n",
      "Epoch: 217/500 Iteration: 2605 Train loss: 0.15080612897872925\n",
      "Epoch: 217/500 Iteration: 2610 Train loss: 0.14020416140556335\n",
      "Epoch: 217/500 Iteration: 2615 Train loss: 0.2509620785713196\n",
      "Epoch: 218/500 Iteration: 2620 Train loss: 0.1273490935564041\n",
      "Epoch: 218/500 Iteration: 2625 Train loss: 0.12782369554042816\n",
      "Validation AUC: 0.6737308429118775\n",
      "Epoch: 219/500 Iteration: 2630 Train loss: 0.1303597390651703\n",
      "Epoch: 219/500 Iteration: 2635 Train loss: 0.042721595615148544\n",
      "Epoch: 219/500 Iteration: 2640 Train loss: 0.16418355703353882\n",
      "Epoch: 220/500 Iteration: 2645 Train loss: 0.06205204501748085\n",
      "Epoch: 220/500 Iteration: 2650 Train loss: 0.1233552098274231\n",
      "Validation AUC: 0.6680435823754789\n",
      "Epoch: 221/500 Iteration: 2655 Train loss: 0.10479702055454254\n",
      "Epoch: 221/500 Iteration: 2660 Train loss: 0.07898120582103729\n",
      "Epoch: 222/500 Iteration: 2665 Train loss: 0.16772174835205078\n",
      "Epoch: 222/500 Iteration: 2670 Train loss: 0.11560767889022827\n",
      "Epoch: 222/500 Iteration: 2675 Train loss: 0.2320931851863861\n",
      "Validation AUC: 0.6724736590038314\n",
      "Epoch: 223/500 Iteration: 2680 Train loss: 0.12625381350517273\n",
      "Epoch: 223/500 Iteration: 2685 Train loss: 0.12296097725629807\n",
      "Epoch: 224/500 Iteration: 2690 Train loss: 0.12411297857761383\n",
      "Epoch: 224/500 Iteration: 2695 Train loss: 0.05204736813902855\n",
      "Epoch: 224/500 Iteration: 2700 Train loss: 0.14783945679664612\n",
      "Validation AUC: 0.6726532567049808\n",
      "Epoch: 225/500 Iteration: 2705 Train loss: 0.05601146072149277\n",
      "Epoch: 225/500 Iteration: 2710 Train loss: 0.1259484589099884\n",
      "Epoch: 226/500 Iteration: 2715 Train loss: 0.08370977640151978\n",
      "Epoch: 226/500 Iteration: 2720 Train loss: 0.08519309014081955\n",
      "Epoch: 227/500 Iteration: 2725 Train loss: 0.14849725365638733\n",
      "Validation AUC: 0.6754669540229884\n",
      "Epoch: 227/500 Iteration: 2730 Train loss: 0.11560685187578201\n",
      "Epoch: 227/500 Iteration: 2735 Train loss: 0.2518013119697571\n",
      "Epoch: 228/500 Iteration: 2740 Train loss: 0.10504274070262909\n",
      "Epoch: 228/500 Iteration: 2745 Train loss: 0.11685320734977722\n",
      "Epoch: 229/500 Iteration: 2750 Train loss: 0.14519137144088745\n",
      "Validation AUC: 0.6754669540229884\n",
      "Epoch: 229/500 Iteration: 2755 Train loss: 0.05520818755030632\n",
      "Epoch: 229/500 Iteration: 2760 Train loss: 0.14054933190345764\n",
      "Epoch: 230/500 Iteration: 2765 Train loss: 0.05871380493044853\n",
      "Epoch: 230/500 Iteration: 2770 Train loss: 0.11564458906650543\n",
      "Epoch: 231/500 Iteration: 2775 Train loss: 0.08324415236711502\n",
      "Validation AUC: 0.676544540229885\n",
      "Epoch: 231/500 Iteration: 2780 Train loss: 0.06976782530546188\n",
      "Epoch: 232/500 Iteration: 2785 Train loss: 0.17043112218379974\n",
      "Epoch: 232/500 Iteration: 2790 Train loss: 0.11021768301725388\n",
      "Epoch: 232/500 Iteration: 2795 Train loss: 0.21036480367183685\n",
      "Epoch: 233/500 Iteration: 2800 Train loss: 0.10671354085206985\n",
      "Validation AUC: 0.675646551724138\n",
      "Epoch: 233/500 Iteration: 2805 Train loss: 0.11344077438116074\n",
      "Epoch: 234/500 Iteration: 2810 Train loss: 0.12724930047988892\n",
      "Epoch: 234/500 Iteration: 2815 Train loss: 0.03773120045661926\n",
      "Epoch: 234/500 Iteration: 2820 Train loss: 0.13785125315189362\n",
      "Epoch: 235/500 Iteration: 2825 Train loss: 0.06690677255392075\n",
      "Validation AUC: 0.6701089559386972\n",
      "Epoch: 235/500 Iteration: 2830 Train loss: 0.10973017662763596\n",
      "Epoch: 236/500 Iteration: 2835 Train loss: 0.09141556918621063\n",
      "Epoch: 236/500 Iteration: 2840 Train loss: 0.0745302215218544\n",
      "Epoch: 237/500 Iteration: 2845 Train loss: 0.16019120812416077\n",
      "Epoch: 237/500 Iteration: 2850 Train loss: 0.10543119162321091\n",
      "Validation AUC: 0.6755866858237548\n",
      "Epoch: 237/500 Iteration: 2855 Train loss: 0.22884060442447662\n",
      "Epoch: 238/500 Iteration: 2860 Train loss: 0.12269768118858337\n",
      "Epoch: 238/500 Iteration: 2865 Train loss: 0.11134614050388336\n",
      "Epoch: 239/500 Iteration: 2870 Train loss: 0.1328049600124359\n",
      "Epoch: 239/500 Iteration: 2875 Train loss: 0.0379788912832737\n",
      "Validation AUC: 0.6751077586206896\n",
      "Epoch: 239/500 Iteration: 2880 Train loss: 0.125316321849823\n",
      "Epoch: 240/500 Iteration: 2885 Train loss: 0.052586719393730164\n",
      "Epoch: 240/500 Iteration: 2890 Train loss: 0.13135170936584473\n",
      "Epoch: 241/500 Iteration: 2895 Train loss: 0.08845868706703186\n",
      "Epoch: 241/500 Iteration: 2900 Train loss: 0.07570523023605347\n",
      "Validation AUC: 0.6733117816091954\n",
      "Epoch: 242/500 Iteration: 2905 Train loss: 0.14851504564285278\n",
      "Epoch: 242/500 Iteration: 2910 Train loss: 0.119242362678051\n",
      "Epoch: 242/500 Iteration: 2915 Train loss: 0.21591268479824066\n",
      "Epoch: 243/500 Iteration: 2920 Train loss: 0.11209974437952042\n",
      "Epoch: 243/500 Iteration: 2925 Train loss: 0.11550470441579819\n",
      "Validation AUC: 0.6749880268199234\n",
      "Epoch: 244/500 Iteration: 2930 Train loss: 0.13211551308631897\n",
      "Epoch: 244/500 Iteration: 2935 Train loss: 0.042981866747140884\n",
      "Epoch: 244/500 Iteration: 2940 Train loss: 0.1035870835185051\n",
      "Epoch: 245/500 Iteration: 2945 Train loss: 0.05215086415410042\n",
      "Epoch: 245/500 Iteration: 2950 Train loss: 0.12731988728046417\n",
      "Validation AUC: 0.671875\n",
      "Epoch: 246/500 Iteration: 2955 Train loss: 0.07924393564462662\n",
      "Epoch: 246/500 Iteration: 2960 Train loss: 0.06883437931537628\n",
      "Epoch: 247/500 Iteration: 2965 Train loss: 0.16693063080310822\n",
      "Epoch: 247/500 Iteration: 2970 Train loss: 0.12152406573295593\n",
      "Epoch: 247/500 Iteration: 2975 Train loss: 0.2365359514951706\n",
      "Validation AUC: 0.6745689655172414\n",
      "Epoch: 248/500 Iteration: 2980 Train loss: 0.1251012235879898\n",
      "Epoch: 248/500 Iteration: 2985 Train loss: 0.12000701576471329\n",
      "Epoch: 249/500 Iteration: 2990 Train loss: 0.11869016289710999\n",
      "Epoch: 249/500 Iteration: 2995 Train loss: 0.03395652398467064\n",
      "Epoch: 249/500 Iteration: 3000 Train loss: 0.15049579739570618\n",
      "Validation AUC: 0.6715756704980842\n",
      "Epoch: 250/500 Iteration: 3005 Train loss: 0.05110786855220795\n",
      "Epoch: 250/500 Iteration: 3010 Train loss: 0.11193006485700607\n",
      "Epoch: 251/500 Iteration: 3015 Train loss: 0.07955444604158401\n",
      "Epoch: 251/500 Iteration: 3020 Train loss: 0.06811247020959854\n",
      "Epoch: 252/500 Iteration: 3025 Train loss: 0.16383537650108337\n",
      "Validation AUC: 0.6740301724137931\n",
      "Epoch: 252/500 Iteration: 3030 Train loss: 0.12273211777210236\n",
      "Epoch: 252/500 Iteration: 3035 Train loss: 0.2481161653995514\n",
      "Epoch: 253/500 Iteration: 3040 Train loss: 0.11302700638771057\n",
      "Epoch: 253/500 Iteration: 3045 Train loss: 0.11730105429887772\n",
      "Epoch: 254/500 Iteration: 3050 Train loss: 0.12185370177030563\n",
      "Validation AUC: 0.6707375478927202\n",
      "Epoch: 254/500 Iteration: 3055 Train loss: 0.04325465112924576\n",
      "Epoch: 254/500 Iteration: 3060 Train loss: 0.129316508769989\n",
      "Epoch: 255/500 Iteration: 3065 Train loss: 0.05110417678952217\n",
      "Epoch: 255/500 Iteration: 3070 Train loss: 0.10903774946928024\n",
      "Epoch: 256/500 Iteration: 3075 Train loss: 0.08269552886486053\n",
      "Validation AUC: 0.6721743295019157\n",
      "Epoch: 256/500 Iteration: 3080 Train loss: 0.0745796337723732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-89227a22514b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Run the graph to get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_auc = []\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        #state = sess.run(initial_state)\n",
    "\n",
    "        # Loop over batches\n",
    "        for ii, (x, y) in enumerate(get_batches(X_train, Y_train, batch_size), 1):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y.astype(np.float32),\n",
    "                    keep_prob_: 0.5,\n",
    "                    learning_rate_: learning_rate}\n",
    "                    #initial_state: state}\n",
    "\n",
    "            # Run the graph to get loss\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {}\".format(loss))\n",
    "            \n",
    "            # Compute validation at every 25 iterations\n",
    "            if iteration%25==0:\n",
    "\n",
    "                # Get initial state for the validation set\n",
    "                #val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                # Compute score on validation set\n",
    "                feed={inputs_:X_valid,\n",
    "                      labels_:Y_valid[:,None].astype(np.float32),\n",
    "                      keep_prob_: 1}\n",
    "                preds = sess.run(predictions, feed_dict=feed)\n",
    "                auc = roc_auc_score(Y_valid[:,None].astype(np.float32), preds)\n",
    "                val_auc.append(auc)\n",
    "                print(\"Validation AUC: {}\".format(auc))\n",
    "                \n",
    "            iteration += 1\n",
    "\n",
    "                \n",
    "    saver.save(sess, \"checkpoints/eeg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb2b850ef60>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUEopBcD/BxVP4S4h9CClAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2be83c9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
